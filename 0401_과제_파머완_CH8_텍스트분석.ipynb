{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과제: 파이썬 머신러닝 완벽 가이드 ch8. 1~3\n",
    "\n",
    "마감: 4월 5일 금요일 18:30\n",
    "\n",
    "pg. 466~486 필사하여 깃허브 주소를 댓글로 남겨주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter8. 텍스트 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NLP (Natural Language Processing) : 텍스트 분석을 향상하게 하는 기반 기술\n",
    "- 텍스트 분석 (Text Analytics, TA) : 비정형 텍스트에서 의미있는 정보를 추출하는 기술. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텍스트 분류 (Text Classification)  : 문서가 특정 분류 또는 카테고리에 속하는 것을 예측하는 기법.\n",
    "- 감성분석 (Sentiment Analysis) : 텍스트에서 나타나는 감정/판단/믿음/의견/기분 등의 주관적인 요소를 분석하는 기법을 총칭한다.\n",
    "- 텍스트 요약 (Summarization) : 텍스트 내에서 중요한 주제나 중심 사상을 추출하는 기법\n",
    "- 텍스트 군집화와 유사도 측정 : 비슷한 유형의 문서에 대해 군집화를 수행하는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## 8-1. 텍스트 분석 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 분석은 비정형 데이터인 텍스트를 분석하는 것이다. 지금까지 ML 모델은 정형 데이터 기반으로 모델을 수립하고 예측을 수행해왔다. 또한 머신러닝 알고리즘은 숫자형의 피처 기반 데이터만 입력받을 수 있기 때문에 텍스트를 머신러닝에 적용하기 위해서는 비정형 데이터를 어떻게 피처 형태로 추출하고, 추출된 피처에 의미있는 값을 부여하는지 여부가 중요하다. \n",
    "- 피처 벡터화(Feature Vectorization) / 피처 추출 (Feature Extraction) : 텍스트를 단어 기반의 피처로 추출하고 피처에 단어 빈도수와 같은 숫자 값을 부여하면 텍스트가 벡터값으로 표현되는 과정\n",
    "- BOW, Word2Vec 방식이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "### 텍스트 분석 수행 프로세스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "머신러닝 기반의 텍스트 분석 프로세스는 다음과 같은 프로세스 순으로 수행된다.\n",
    "1. **텍스트 사전 준비작업 (텍스트 전처리)** : 텍스트를 피처로 만들기 전에 클렌징, 대/소문자 변경, 특수문자 삭제, 토큰화 작업, 의미 없는 단어(Stop word) 제거 작업, 어근 추출 (Stemming/Lemmatization) 등의 텍스트 정규화 작업을 수행하는 것을 말한다.\n",
    "2. **피처 벡터화 / 추출** : 사전 준비 작업으로 가공된 텍스트에서 피처를 추출하고 여기에 벡터값을 할당한다.\n",
    "3. **ML 모델 수립 및 학습/예측/평가** : 피처 벡터화된 데이터 세트에 ML 모델을 적용해 학습/예측 및 평가를 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "### 파이썬 기반의 NLP, 텍스트 분석 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NLTK : 파이썬의 가장 대표적인 NLP 패키지. 방대한 데이터 세트와 모듈을 가지고 있으며 NLP의 거의 모든 영역을 커버하고 있다. 그러나 수행 속도 측면에서 부족한 부분이 있어 실제 대량의 데이터 기반에서는 제대로 활용되지 못하고 있다.\n",
    "\n",
    "- Gensim : 토픽 모델링 분야에서 가장 두각을 나타내는 패키지. SpaCy와 함께 가장 많이 사용되는 NLP 패키지이다. \n",
    "\n",
    "- SpaCy : 뛰어난 수행 능력으로 최근 가장 주목을 받는 NLP 패키지이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## 8-2. 텍스트 사전 준비 작업 (텍스트 전처리) - 텍스트 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 정규화 작업은 크게 다음과 같이 분류된다.\n",
    "- 클렌징\n",
    "- 토큰화\n",
    "- 필터링 / 스톱 워드 제거 / 철자 수정\n",
    "- Stemming\n",
    "- Lemmatization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "### [1] 텍스트 토큰화\n",
    "텍스트 토큰화는 다음 두 가지 유형으로 나뉜다.\n",
    "- 문장 토큰화 : 문서에서 문장을 분리\n",
    "- 단어 토큰화 : 문장에서 단어를 토큰으로 분리\n",
    "\n",
    "#### 1) 문장 토큰화 (sentence tokenization)\n",
    "문장의 마침표(.), 개행문자(\\n) 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것이 일반적이다.\n",
    "또는 정규 표현식에 따른 문장 토큰화도 가능하다.\n",
    "NLTK에서 일반적으로 많이 쓰이는 \"**sent_tokenize**\"를 이용하여 토큰화를 실시해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\홍예원\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "                You can see it out your window or on your television. \\\n",
    "                You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text = text_sample)\n",
    "\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sent_tokenize()가 반환한 것은 list 객체로, 문장으로 인식된 문자열이 들어있다. 반환된 list 객체가 3개의 문장으로 된 문자열을 가지고 있는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 단어 토큰화 (Word Tokenization)\n",
    "\n",
    "공백, 콤마(,), 마침표(.), 개행문자 등으로 단어를 분리하지만, 정규표현식을 이용해 다양한 유형으로 토큰화를 수행할 수도 있다.\n",
    "\n",
    "마침표나 개행문자와 같이 문장을 분리하는 구분자를 이용해 단어를 토큰화 할 수도 있으므로 Bag of Word와 같이 단어의 순서가 중요하지 않은 경우 문장 토큰화를 사용하지 않고 단어 토큰화만 사용해도 충분하다. 일반적으로 문장 토큰화는 각 문장이 가지는 symantic한 의미가 중요한 요소로 쓰일 때 사용한다. NLTK에서 기본으로 제공하는 \"**work_tokenize**\"를 이용해 단어로 토큰화해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 sent_tokenize와 word_tokenize를 조합해 문서에 대해서 모든 단어를 토큰화해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하게 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "\n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "# 여러 문장에 대해 문장별 단어 토큰화 수행\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3개의 문장을 문장별로 토큰화했으므로 word_tokens는 3개의 리스트 객체를 갖는 리스트이다. 포함된 개별 리스트 객체는 각 문장에 대해 토큰화된 단어를 갖는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "### 스톱워드 제거\n",
    "\n",
    "\"스톱 워드\"는 분석에 큰 의미가 없는 단어를 지칭한다. 영어에서는 is, the, a, will 등 문장을 구성하는 필소요소이지만 문맥적으로 큰 의미가 없는 단어들이 해당된다. 이런 단어들은 빈번하게 텍스트에 나타나므로 사전에 제거하지 않으면 오히려 빈번함으로 인해 중요한 단어로 인식될 수 있다.\n",
    "\n",
    "언어별로 이러한 스톱 워드가 목록화 되어있다. NLTK의 경우 가장 다양한 언어의 스톱 워드를 제공한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\홍예원\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수 :  179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "print('영어 stop words 개수 : ', len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영어의 경우 스톱 워드의 개수가 179개이다. 이를 예시에 적용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "# 위 예제에서 3개의 문장별로 얻은 word_tokens list에 대해 스톱워드를 제거하는 반복문\n",
    "for sentence in word_tokens :\n",
    "    filtered_words = []\n",
    "    # 개별 문장별로 토큰화된 문장 list에 대해 스톱 워드를 제거하는 반복문\n",
    "    for word in sentence :\n",
    "        # 소문자로 모두 변환한다.\n",
    "        word = word.lower()\n",
    "\n",
    "        # 토큰화된 개별 단어가 스톱워드의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords :\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "### Stemming과 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영어에서 work라는 단어는 과거형은 worked, 3인칭 단수형은 works, 진행형은 working 등 다양하게 변한다. Stemming과 Lemmatization은 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 것이다.\n",
    "\n",
    "\n",
    "- Stemming : 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용해 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출하는 경향이 있다.\n",
    "- Lemmatization : Stemming보다 정교하고 의미론적으로 단어의 원형을 찾는다. 품사 같은 문법 요소나 의미적인 부분을 고려하여 정확한 철자의 어근 단어를 찾는다. 시간이 Stemming보다 더 오래 걸리는 경향이 있다.\n",
    "\n",
    "<클래스>\n",
    "- Stemmer : Porter, Lancaster, Snowvall Stemmer\n",
    "- Lemmatization : WorkNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Stemming : Stemmer 객체를 생성한 뒤 이 객체의 stem('단어') 메서드를 호출하면 원하는 '단어'의 Stemming을 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. work : working, works, worked 모두 원형인 work로 인식한다.\n",
    "2. amuse : amus를 원형으로 인식한다.\n",
    "3. happy : 비교급인 happier는 원형을 인식하였으나 최상급인 happiest는 원형을 찾지 못하고 어근 단어로 인식하였다.\n",
    "4. fancy : 비교급과 최상급 모두 원형인 fancy로 인식하지 못하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "Lemmatization : 보다 정확한 원형 단어 추출을 위해 단어의 품사를 입력해야한다. lemmatize()의 파라미터로 동사는 'v', 형용사는 'a'를 입력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\홍예원\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
    "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
    "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "## 8-2. Bag of Words - BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words : 문맥이나 순서를 무시하고 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델\n",
    "\n",
    "장점 \n",
    "- 쉽고 빠른 구축\n",
    "- 단순히 단어의 발생 횟수에 기반하고 있지만, 예상보다 문서의 특징을 잘 나타낸다.\n",
    "\n",
    "단점\n",
    "- 문맥 의미 (Semantic Context) 반영 부족\n",
    "- 희소 행렬 문제 (희소성, 희소 행렬) : BOW로 피처 벡터화를 수행하면 희소 행렬 형태의 데이터 세트가 만들어지기 쉽다. 많은 문서에서 단어를 추출하면 매우 많은 단어가 칼럼으로 만들어진다. 하나의 문서에만 있는 단어는 이 중 극히 일부분이므로 대부분의 데이터가 0으로 채워진다. 이처럼 대규모의 칼럼으로 구성된 행렬에서 대부분의 값이 0으로 채워지는 행렬을 **희소 행렬 (Sparse Matrix)**라고 한다. 반대로 대부분의 값이 0이 아닌 의미있는 값으로 채워진 행렬을 **밀집 행렬 (Dense Matrix)**이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "### BOW 피처 벡터화\n",
    "\n",
    "BOW 피처 벡터화에는 다음 두 가지 방식이 있다.\n",
    "1. 카운트 기반의 벡터화 : 단어가 나타나면 count를 부여하는 방법. 그러나 카운트만 부여할 경우 그 문서의 특징을 나타내기보다는 언어의 특성상 문장에서 자주 사용될 수밖에 없는 단어까지 높은 값을 부여하게 된다는 문제점이 있다.\n",
    "2. TF-IDF (Term Frequency - Inverse Document Frequency) 기반의 벡터화 : 개별 문서에서 자주 나타나는 단어에 높은 가중치를 주되, 모든 문서에서 전반적으로 자주 나타나는 단어에 패널티를 주는 방법.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "### 사이킷런의 Count 및 TF-IDF 벡터화 구현 : CountVectorizer, TfidVectorizer\n",
    "CountVectorizer 클래스는 피처 벡터화를 포함하여 소문자 일괄 변환, 토큰화, 스톱 워드 필터링 등의 텍스트 전처리도 수행한다. 다음과 같은 파라미터들을 설정하여 동작시킬 수 있다.\n",
    "\n",
    "- max_df : 너무 높은 빈도수를 가지는 단어 피처를 제외하기 위한 파라미터. max_df = 100처럼 정수값을 입력하면 전체 문서에 걸쳐 100번 이하로만 나타나는 단어만 추출한다. max_df = 0.95와 같이 소수점을 가지면 전체 문서에 걸쳐 빈도수 0~95% 까지의 단어만 피처로 추출한다.\n",
    "- min_df : 너무 낮은 빈도수를 가지는 단어 피처를 제외하기 위한 파라미터. min_df = 2처럼 정수값을 입력하면 전체 문서에 걸쳐 2번 이하로 나타나는 단어는 추출하지 않는다. min_df = 0.02와 같이 소수점을 가지면 전체 문서에 걸쳐 빈도수 하위 2%의 단어들은 피처로 추출하지 않는다.\n",
    "- max_features : 추출하는 피처의 개수를 제한한다.\n",
    "- stop_words : 'english'로 지정하면 영어의 스톱워드로 지정된 단어는 추출에서 제외한다.\n",
    "- n_gram_range : tuple 형태로 (범위 최솟값, 범위 최댓값)을 지정한다.\n",
    "- analyzer : 피처 추출을 수행한 단위. 디폴트는 'word'\n",
    "- token_pattern : 토큰화를 수행하는 정규표현식 패턴. 디폴트는 '\\b\\w\\w+\\b'\n",
    "- tokenizer : 토큰화를 별도의 커스텀 함수로 이용시 적용\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 희소 행렬 - COO 형식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COO (Coordinate : 좌표) 형식은 0이 아닌 데이터만 별도의 데이터 배열 (Array)에 저장하고, 그 데이터가 가리키는 행과 열의 위치를 별도의 배열로 저장하는 방식\n",
    "\n",
    "파이썬에서는 희소 행렬 변환을 위해 주로 Scipy의 sparse 패키지를 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dense = np.array([[3,0,1], [0,2,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 밀집 행렬을 COO 형식의 희소 행렬로 변환해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 배열로 생성\n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix를 이용해 COO 형식으로 희소 행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "### 희소 행렬 - CSR 형식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSR (Compressed Sparse Row) 형식은 CPP 형식이 행과 열의 위치를 나타내기 위해 반복적인 위치 데이터를 사용해야 하는 문제점을 해결한 방식이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- COO 변환 형식의 문제점\n",
    ": 순차적인 같은 값이 반복적으로 나타난다. 행 위치 배열이 0부터 순차적으로 증가하는 값으로 이뤄졌다는 특성을 고려하면 행 위치 배열의 고유한 값의 시작 위치만 표기하는 방법으로 이러한 반복을 제거할 수 있다. (위치의 위치를 나타내는 방법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense = np.array([[0,0,1,0,0,5],\n",
    "                  [1,4,0,3,2,5],\n",
    "                  [0,6,0,3,0,0],\n",
    "                  [2,0,0,0,0,0],\n",
    "                  [0,0,0,7,0,8],\n",
    "                  [1,0,0,0,0,0]\n",
    "\n",
    "])\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성\n",
    "row_pos = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
    "col_pos = np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
    "\n",
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0,2,7,9,10,12,13])\n",
    "\n",
    "# CSR 형식으로 변환\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray())\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense3= np.array([[0,0,1,0,0,5],\n",
    "                  [1,4,0,3,2,5],\n",
    "                  [0,6,0,3,0,0],\n",
    "                  [2,0,0,0,0,0],\n",
    "                  [0,0,0,7,0,8],\n",
    "                  [1,0,0,0,0,0]\n",
    "\n",
    "])\n",
    "\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.csr_matrix(dense3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
